<!DOCTYPE html>
<html>
<head>
<title>Kevin Eppacher - Github IO</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=0.9">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
<link rel="stylesheet" href="./src/styles.css">
<link rel="shortcut icon" type="image/x-icon" href="/favicon.ico"> <!--http://tools.dynamicdrive.com/favicon/-->

</head>
<body class="w3-light-grey">

<!-- w3-content defines a container for fixed size centered content, 
and is wrapped around the whole page content, except for the footer in this example -->
<div class="w3-content">

<!-- Grid -->
<div class="w3-row">

<!-- Introduction menu -->
<div class="w3-col l4">
  <!-- About Card -->
  <div class="w3-card w3-margin w3-margin-top">
    <a href="./index.html">
      <img src="./img/portrait4.jpg" style="width:100%; border-top-left-radius: 20px; border-top-right-radius: 20px;">
    </a>
    <div class="w3-container w3-white" style="border-top-left-radius: 0px; border-top-right-radius: 0px;">
      <a href="./index.html">
        <h4><b>Kevin Eppacher, B.Sc.</b></h4>
      </a>
      
      <p>Robotics Software Engineer specializing in perception-driven autonomy, semantic mapping, and intelligent control. Focused on developing systems that combine vision, optimization, and learning for real-world robotic applications.</p>

      <ul style="list-style-type:none; padding:0px; margin:0px;">
        <hr>

        <li>
          <a href="./cv/KevinEppacherCV.pdf" target="_blank">
            <button class="w3-button w3-padding-16 w3-white w3-block w3-left-align">
              <img style="height:20px; transform:translate(-50%,-2.5px);" src="./img/icons/briefcase.svg"><span><b>CV PDF Download</b></span></b>
            </button>
          </a>
        </li>

        <li>
          <a href="./portfolio.html">
            <button class="w3-button w3-padding-16 w3-white w3-block w3-left-align">
              <img style="height:20px; transform:translate(-50%,-2.5px);" src="./img/icons/square-terminal.svg"><span><b>Portfolio</b></span></b>
            </button>
          </a>
        </li>

        <hr>

        <li>
          <a href="https://github.com/KevinEppacher" target="_blank">
            <button class="w3-button w3-padding-16 w3-white w3-block w3-left-align">
              <img style="height:20px; transform:translate(-50%,-2.5px);" src="./img/icons/github.svg"><span><b>GitHub</b></span></b>
            </button>
          </a>
        </li>

        <li>
          <a href="https://www.linkedin.com/in/kevin-eppacher-4464591a0/" 
            target="_blank" 
            class="w3-button w3-padding-16 w3-white w3-block w3-left-align">
            <img style="height:20px; transform:translate(-40%,-2.5px);" src="./img/icons/linkedin.svg">
            <span><b>LinkedIn</b></span>
          </a>
        </li>

      </ul>
      <p></p>
    </div>
  </div>
  
<!-- END Introduction Menu -->
</div>

<!-- Information Cards -->
<div class="w3-col l8 s12">

  <!-- Content Class -> Holds one Publication -->
  <div class="w3-card-4 w3-margin w3-white" style="padding: 15pt;">
    
    <!-- START GENERATED HTML HERE --><h1>Professional Work</h1>
<p>A curated selection of my robotics research and engineering work, focused on real-time control, 3D perception, optimization, and applied AI-driven autonomy.</p>
<h1>Table of Contents</h1>
<ul>
<li><a href="#sage">1. SAGE ‚Äì Semantic-Aware Guided Exploration</a></li>
<li><a href="#rl-framework">2. ROS 2 Reinforcement Learning Framework</a></li>
<li><a href="#nmpc-casadi">3. Nonlinear MPC (CasADi/IPOPT)</a></li>
<li><a href="#mpc-pytorch">4. PyTorch-Based MPC</a></li>
<li><a href="#airskin">5. AIRSKIN Sensitivity Measurement System</a></li>
<li><a href="#mcl">6. Monte Carlo Localization</a></li>
<li><a href="#pt-uav">7. Pan‚ÄìTilt UAV Tracking</a></li>
<li><a href="#personal-projects">8. Personal Projects</a></li>
</ul>
<p><a id="sage"></a></p>
<h2><strong>1. SAGE ‚Äì Semantic-Aware Guided Exploration with Persistent Memory (Master Thesis)</strong></h2>
<p>A hybrid semantic exploration framework for <strong>multi-object search with persistent memory</strong>, integrating <strong>vision-language models</strong>, <strong>semantic mapping</strong>, and <strong>frontier-based navigation</strong> for intelligent exploration and reasoning.</p>
<center>
  <video id="searchVideo" width="70%" controls autoplay loop muted>
    <source src="./videos/search_fridge.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>

  <figcaption style="font-size:0.9em; color:gray; margin-top:6px;">
    Robot searching for text prompt: <em>"fridge"</em>
  </figcaption>

  <script>
    const v = document.getElementById('searchVideo');
    v.playbackRate = 10.0;  // plays 10√ó faster
  </script>

</center>

<div align="center" style="display:flex; justify-content:center; gap:20px; flex-wrap:wrap;">
  <figure style="margin:0;">
    <img src="./img/somewhere_to_sleep.png" width="450" style="border-radius:8px;">
    <figcaption style="font-size:0.9em; color:gray; margin-top:5px;">
      Persistent 3D semantic memory representation for text prompt "somewhere  to sleep"
    </figcaption>
  </figure>

  <figure style="margin:0;">
    <img src="./img/isaac_sim_demo.png" width="450" style="border-radius:8px;">
    <figcaption style="font-size:0.9em; color:gray; margin-top:5px;">
      Semantic exploration in Isaac Sim environment
    </figcaption>
  </figure>
</div>

<hr />
<details>
<summary>üìò Read full description</summary>

<h3>Description</h3>
<p><strong>SAGE (Semantic-Aware Guided Exploration)</strong> is a framework designed for <strong>multi-object search</strong> in unknown environments using <strong>persistent 3D semantic memory</strong>.<br />
It combines <strong>exploration</strong>, <strong>semantic understanding</strong>, and <strong>memory-based reasoning</strong> to enable robots to search and identify objects efficiently using open-vocabulary prompts.</p>
<p>The system integrates multiple AI and robotics components:</p>
<ul>
<li><strong>OpenFusion</strong> as a 3D semantic SLAM mapper, acting as persistent memory for detected objects.</li>
<li><strong>Frontier-based exploration</strong> for geometric expansion of the map, enhanced by a <strong>Vision-Language Model (VLM)</strong> scoring system to evaluate which frontiers are most likely to contain queried objects.</li>
<li><strong>YOLO-E</strong> for real-time object detection and <strong>BLIP-2</strong> for multimodal grounding, fused with <strong>OpenFusion‚Äôs semantic map</strong> for robust and context-aware detection.</li>
<li>The combination of <strong>VLM-based reasoning</strong> and <strong>semantic memory</strong> allows the system to continuously refine its understanding of the environment and improve future searches.</li>
</ul>
<p><strong>Evaluation:</strong><br />
To validate SAGE, 3D semantic segmentation with OpenFusion is used to compare object detection and mapping accuracy against the same semantic classes.<br />
Performance is measured using <strong>Success Rate (SR)</strong> and <strong>Success weighted by Path Length (SPL)</strong> metrics for single and multi-object search tasks.</p>
<center>
  <video id="evalVideo" width="70%" controls autoplay loop muted>
    <source src="./videos/evaluation_pcl.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>

  <figcaption style="font-size:0.9em; color:gray; margin-top:6px; max-width:70%;">
    Dynamic semantic evaluation map filtering for <em>chair</em>, <em>TV</em>, <em>sofa</em>, and <em>fridge</em>,
    continuously calculating the shortest path to the nearest object.
  </figcaption>

  <script>
    const v = document.getElementById('evalVideo');
    v.playbackRate = 3.0;  // plays at 3√ó speed
  </script>

</center>

<h3>Key Features</h3>
<ul>
<li>Designed a full multi-modal semantic mapping pipeline combining depth, RGB, VLM reasoning, and 3D fusion.</li>
<li>Implemented a persistent semantic memory layer using OpenFusion‚Äôs voxel representation.</li>
<li>Developed frontier-based navigation with semantic scoring for multi-object search.</li>
<li>Integrated YOLO-E, BLIP-2, and SEEM into real-time ROS 2 perception pipelines.</li>
<li>Built a full evaluation pipeline using SR/SPL across multiple object classes.</li>
</ul>
<p>The project is currently under <strong>active development</strong>, with further experiments in <strong>semantic fusion</strong>, <strong>frontier optimization</strong>, and <strong>real-world deployment</strong> in progress.</p>
<hr />
<h3>Frameworks &amp; Tools</h3>
<p><img src="https://img.shields.io/badge/ROS2-Humble-blue?logo=ros" alt="ROS 2" /><br />
<img src="https://img.shields.io/badge/OpenFusion-3D%20Semantic%20Mapping-purple" alt="OpenFusion" /><br />
<img src="https://img.shields.io/badge/YOLO--E-Zero--Shot%20Detection-red" alt="YOLO-E" /><br />
<img src="https://img.shields.io/badge/BLIP2-Vision--Language%20Model-orange" alt="BLIP2" /><br />
<img src="https://img.shields.io/badge/SEEM-Segment%20Everything%20Everywhere-blueviolet" alt="SEEM" /><br />
<img src="https://img.shields.io/badge/Nav2-Frontier%20Exploration-brightgreen?logo=ros" alt="Nav2" /><br />
<img src="https://img.shields.io/badge/Isaac--Sim-Simulation-lightgrey?logo=nvidia" alt="Isaac Sim" /><br />
<img src="https://img.shields.io/badge/Python-3.10-yellow?logo=python" alt="Python" /><br />
<img src="https://img.shields.io/badge/Docker-Reproducibility-blue?logo=docker" alt="Docker" /></p>
<hr />
<h3>Links</h3>
<ul>
<li><a href="https://github.com/KevinEppacher/SAGE.git">GitHub Repository</a></li>
</ul>
<h2>https://github.com/KevinEppacher/SAGE.git</h2>
<h3>Summary</h3>
<p><strong>SAGE</strong> introduces a semantic exploration architecture that fuses <strong>frontier-based exploration</strong>, <strong>3D mapping</strong>, and <strong>vision-language models</strong> into a unified pipeline for <strong>open-vocabulary multi-object search</strong>.<br />
Through <strong>persistent semantic memory</strong> and <strong>cross-modal fusion</strong>, it enables robots to recall, reason, and plan toward objects intelligently during long-term autonomous missions.</p>
</details>

<hr />
<p><a id="rl-framework"></a></p>
<h2><strong>2. ROS 2 Reinforcement Learning Framework</strong></h2>
<p>A modular ROS 2 reinforcement-learning framework built for real-time robotics applications, enabling vectorized training, live introspection, and plug-in environments for reproducible DRL research.</p>
<center>
<img src="./videos/car_racing_demo.gif" width="70%">
</center>

<hr />
<details>
<summary>üìò Read full description</summary>

<h3>Description</h3>
<p>A modular <strong>ROS 2 Deep Reinforcement Learning (DRL) framework</strong> developed as a <strong>commissioned project</strong> to provide a standardized, extensible platform for end-to-end learning in robotics.<br />
The goal was to <strong>lower the entry barrier for students and research teams</strong> by enabling quick prototyping, reproducible training, and real-time introspection within ROS 2.</p>
<p>The framework integrates tightly with <strong>Stable-Baselines3</strong> and supports <strong>plug-in-based environments</strong>, allowing new tasks to be added without modifying the RL core.<br />
It comes with practical examples (CarRacing, LunarLander, CartPole) and extensive documentation covering <strong>observation/action space design</strong>, <strong>reward shaping</strong>, and <strong>hyperparameter tuning</strong>.</p>
<p>It also supports <strong>vectorized environments</strong> for parallel training and can <strong>introspect live ROS 2 topics during learning</strong>, enabling developers to visualize and debug agent behavior in real time, as shown below.</p>
<center>
<img src="./img/vec_env_car_racing_training.png" width="70%" style="margin:10px;">
</center>

<p>The framework emphasizes <strong>reproducibility, scalability, and transparency</strong>, making it an ideal foundation for both industrial and educational reinforcement-learning applications.</p>
<hr />
<h3>Key Features</h3>
<ul>
<li>Unified training and evaluation pipeline for ROS 2 environments</li>
<li>Plug-in architecture for easily registering new environments</li>
<li>TensorBoard integration and live ROS 2 topic publishing during training</li>
<li>Supports vectorized environments for high-throughput parallel learning</li>
<li>Real-time introspection of published ROS 2 topics to monitor agent behavior</li>
<li>GPU-accelerated PPO, SAC, and TD3 training support</li>
<li>Ready-to-use environments (CartPole, LunarLander, CarRacing)</li>
<li>Clear YAML-based configuration for all algorithms</li>
</ul>
<hr />
<h3>Frameworks &amp; Tools</h3>
<p><img src="https://img.shields.io/badge/ROS2-Jazzy-blue?logo=ros" alt="ROS 2" /><br />
<img src="https://img.shields.io/badge/Stable--Baselines3-DRL%20Library-orange" alt="Stable-Baselines3" /><br />
<img src="https://img.shields.io/badge/PyTorch-2.2-red?logo=pytorch" alt="PyTorch" /><br />
<img src="https://img.shields.io/badge/Gymnasium-Environments-lightgrey?logo=openai" alt="Gymnasium" /><br />
<img src="https://img.shields.io/badge/Python-3.10-yellow?logo=python" alt="Python" /><br />
<img src="https://img.shields.io/badge/TensorBoard-Visualization-orange?logo=tensorflow" alt="TensorBoard" /><br />
<img src="https://img.shields.io/badge/Docker-Reproducibility-blue?logo=docker" alt="Docker" /></p>
<p><em>Built and tested under ROS 2 Jazzy with CUDA-enabled PyTorch 2.2 for GPU training.</em></p>
<hr />
<h3>Links</h3>
<ul>
<li><a href="https://github.com/KevinEppacher/ros2_reinforcement_learning_framework.git">GitHub Repository</a></li>
</ul>
<hr />
<h3>Summary</h3>
<p>A professionally developed <strong>ROS 2 reinforcement learning framework</strong> unifying algorithm design, training, and evaluation in robotics.<br />
It bridges <strong>educational usability</strong> and <strong>research-grade scalability</strong>, empowering students, researchers, and engineers to prototype and deploy intelligent robotic behaviors efficiently.</p>
</details>

<hr />
<p><a id="nmpc-casadi"></a></p>
<h2><strong>3. Nonlinear Model Predictive Controller (nMPC) for Differential Drive Mobile Robot</strong></h2>
<p>A high-precision nonlinear control system for differential-drive robots that predicts future motion and optimizes control inputs over a finite horizon, enabling smooth constraint-aware trajectory tracking.</p>
<center>
<video width="70%" controls autoplay loop muted>
  <source src="./videos/nMPC.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</center>

<hr />
<details>
<summary>üìò Read full description</summary>

<h3>Description</h3>
<p>A <strong>nonlinear Model Predictive Controller (nMPC)</strong> based local planner developed for a <strong>Differential Drive Mobile Robot (DDMR)</strong>.</p>
<p>Unlike conventional reactive planners, the nMPC predicts future robot states through a <strong>kinematic model</strong> and optimizes control inputs over a finite horizon.<br />
The controller minimizes a cost function while enforcing <strong>hard constraints</strong> on obstacle clearance, velocity, and input bounds.</p>
<p>The implementation leverages:</p>
<ul>
<li><strong>CasADi</strong> for nonlinear optimization formulation</li>
<li><strong>IPOPT</strong> (Interior Point Optimizer) for efficient real-time solving</li>
<li><strong>Python / ROS Noetic</strong> for seamless runtime integration</li>
</ul>
<p>The planner was benchmarked against standard local planners:</p>
<ul>
<li><strong>Dynamic Window Approach (DWA)</strong></li>
<li><strong>Timed Elastic Band (TEB)</strong></li>
</ul>
<p>Results demonstrate smoother, dynamically feasible trajectories, particularly in cluttered or narrow environments.<br />
The entire system was simulated in <strong>Gazebo</strong> using a <strong>TurtleBot</strong>, with a GPU-enabled <strong>Docker</strong> container for reproducibility.</p>
<blockquote>
<p><em>See also the <a href="#3-optimization-lab-pytorch-based-mpc-for-robotics">Optimization Lab ‚Äì PyTorch-based MPC (ROS 2)</a> for a lightweight educational re-implementation using PyTorch instead of CasADi.</em></p>
</blockquote>
<hr />
<h3>Frameworks &amp; Libraries</h3>
<p><img src="https://img.shields.io/badge/ROS-Noetic-blue?logo=ros" alt="ROS Noetic" /><br />
<img src="https://img.shields.io/badge/Nav2-Navigation-brightgreen?logo=ros" alt="Nav2" /><br />
<img src="https://img.shields.io/badge/CasADi-Optimization-orange" alt="CasADi" /><br />
<img src="https://img.shields.io/badge/IPOPT-Solver-lightgrey" alt="IPOPT" /><br />
<img src="https://img.shields.io/badge/Python-3.10-yellow?logo=python" alt="Python" /><br />
<img src="https://img.shields.io/badge/Gazebo-Simulation-blueviolet?logo=ros" alt="Gazebo" /><br />
<img src="https://img.shields.io/badge/Docker-Containerization-blue?logo=docker" alt="Docker" /></p>
<hr />
<h3>Links</h3>
<ul>
<li><a href="https://github.com/KevinEppacher/walle_ws.git">GitHub Repository</a></li>
<li><a href="./papers/nMPC.pdf">Download unpublished Research Paper (PDF)</a></li>
</ul>
<hr />
<h3>Summary</h3>
<p>A high-performance nonlinear MPC for mobile robots using CasADi and IPOPT delivering smooth constraint-aware motion planning and serving as a foundation for subsequent PyTorch-based re-implementations in ROS 2.</p>
</details>

<hr />
<p><a id="mpc-pytorch"></a></p>
<h2><strong>4. Optimization Lab ‚Äì PyTorch-Based MPC for Robotics</strong></h2>
<p>An educational ROS 2 lab demonstrating real-time control through gradient-based optimization with PyTorch, teaching how to implement MPC without external solvers.</p>
<center>
<img src="./videos/mpc_demo_1.gif" width="70%">
</center>

<hr />
<details>
<summary>üìò Read full description</summary>

<h3>Description</h3>
<p>A <strong>PyTorch-based Model Predictive Control (MPC)</strong> framework developed as part of a <strong>university optimization lab</strong>, demonstrating how numerical optimization can be applied to control and planning problems in robotics.<br />
Unlike the earlier CasADi-based MPC, this version leverages <strong>PyTorch autograd and optimizers (Adam/LBFGS)</strong> directly, without relying on external NLP solvers, to teach students how to <em>formulate and solve control problems from first principles</em>.</p>
<p>Developed as a <strong>commissioned project</strong>, the lab provides a complete <strong>ROS 2 Jazzy package (<code>mpc_local_planner</code>)</strong> that serves as both a tutorial and a working local planner.<br />
It includes comprehensive documentation explaining:</p>
<ul>
<li>optimization in localization, planning, and control,</li>
<li>MPC cost shaping and constraints,</li>
<li>and real-time optimization loops using PyTorch tensors.</li>
</ul>
<hr />
<h3>Key Highlights</h3>
<ul>
<li>Created an <strong>Optimization Lab</strong> for the <em>UAS Technikum Vienna</em> robotics curriculum</li>
<li>Developed a <strong>didactic ROS 2 Jazzy package</strong>: <code>mpc_local_planner</code></li>
<li>Implemented <strong>nMPC with PyTorch optimizers</strong> (Adam, LBFGS, RMSProp)</li>
<li>Demonstrated <strong>gradient-based MPC without CasADi/IPOPT</strong></li>
<li>Integrated with <strong>Nav2</strong> for trajectory tracking using costmap penalties</li>
<li>Provided <strong>extensive documentation, code comments, and exercises</strong></li>
<li>Used in teaching labs to show <strong>real-time control, optimization, and differentiable robotics</strong> concepts</li>
</ul>
<hr />
<h3>Frameworks &amp; Tools</h3>
<p><img src="https://img.shields.io/badge/ROS2-Jazzy-blue?logo=ros" alt="ROS 2" /><br />
<img src="https://img.shields.io/badge/Nav2-Navigation-brightgreen?logo=ros" alt="Nav2" /><br />
<img src="https://img.shields.io/badge/PyTorch-2.2-red?logo=pytorch" alt="PyTorch" /><br />
<img src="https://img.shields.io/badge/Python-3.10-yellow?logo=python" alt="Python" /><br />
<img src="https://img.shields.io/badge/Optimizers-Adam%2FLBFGS%2FRMSProp-lightgrey" alt="TorchOptimizer" /><br />
<img src="https://img.shields.io/badge/Docker-Reproducibility-blue?logo=docker" alt="Docker" /></p>
<p><em>Built and tested under ROS 2 Jazzy using CUDA-enabled PyTorch 2.2.</em></p>
<hr />
<h3>Links</h3>
<ul>
<li><a href="https://github.com/KevinEppacher/mpc_local_planner.git">GitHub Repository</a></li>
</ul>
<hr />
<h3>Summary</h3>
<p>A <strong>university lab project</strong> showcasing optimization for robotics using <strong>PyTorch as a numerical solver</strong>.<br />
It bridges classical control and differentiable programming by re-implementing nMPC entirely in PyTorch, illustrating how learning-based and optimization-based control can converge within modern ROS 2 pipelines.</p>
</details>

<hr />
<p><a id="airskin"></a></p>
<h2><strong>5. Automated Sensitivity Measurement System (AIRSKIN)</strong></h2>
<p>Designed and implemented an automated force‚Äìdisplacement measurement system using a UR10 robot, FT sensor, and RGB-D visualization, enabling reproducible AIRSKIN pad calibration.</p>
<center>
<video width="70%" controls autoplay loop muted>
  <source src="./videos/sensibility_measurements.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</center>

<hr />
<details>
<summary>üìò Read full description</summary>

<h3>Description</h3>
<p>A <strong>collaborative project with <a href="https://www.airskin.io/">Blue Danube Robotics ‚Äì AIRSKIN</a></strong> developed at <strong>UAS Technikum Vienna</strong> to automate tactile pad sensitivity measurements.</p>
<p>The system measures the <strong>force and displacement</strong> required to trigger an AIRSKIN pad at defined grid points. From this, the <strong>spring constant</strong> and <strong>local sensitivity</strong> are derived to detect mechanical weak points and support further product development.</p>
<p>Built entirely with <strong>ROS Noetic</strong> and <strong>Docker</strong>, the system integrates:</p>
<ul>
<li>A <strong>UR10</strong> industrial robot</li>
<li>A <strong>force‚Äìtorque (FT) sensor</strong> connected via the UR ROS bridge (TCP/IP)</li>
<li>A <strong>custom ImGui C++ HMI</strong> for switching between <em>Freedrive Mode</em> and <em>External Control Mode</em></li>
</ul>
<p>Once all measurement points are defined, <strong>MoveIt</strong> executes a fully automated sequence. The system visualizes force vectors in <strong>RViz</strong> and overlays a 3D point cloud from an integrated RGB-D camera.</p>
<hr />
<h3>Frameworks &amp; Libraries</h3>
<p><img src="https://img.shields.io/badge/ROS-Noetic-blue?logo=ros" alt="ROS Noetic" /><br />
<img src="https://img.shields.io/badge/MoveIt-Motion%20Planning-purple?logo=ros" alt="MoveIt" /><br />
<img src="https://img.shields.io/badge/ImGui-C%2B%2B%20GUI-lightgrey" alt="ImGui" /><br />
<img src="https://img.shields.io/badge/RViz-Visualization-orange?logo=ros" alt="RViz" /><br />
<img src="https://img.shields.io/badge/Gazebo-Simulation-blueviolet?logo=ros" alt="Gazebo" /><br />
<img src="https://img.shields.io/badge/Docker-Containerization-blue?logo=docker" alt="Docker" /><br />
<img src="https://img.shields.io/badge/UR--ROS--Driver-UR10%20Control-darkblue" alt="UR ROS Driver" /></p>
<hr />
<h3>Links</h3>
<ul>
<li><a href="https://github.com/KevinEppacher/goldilocks_sensibility_ws.git">GitHub Repository</a></li>
</ul>
<hr />
<h3>Summary</h3>
<p>Automated robotic test bench for AIRSKIN pad calibration, measuring and visualizing tactile sensitivity through force‚Äìdisplacement mapping.</p>
</details>

<hr />
<p><a id="mcl"></a></p>
<h2><strong>6. Monte Carlo Localization (Particle Filter) for Mobile Robots</strong></h2>
<p>Custom particle filter for 2D localization with optimized raycasting and resampling, achieving reliable pose estimation with only 100 particles.</p>
<center>
<img src="./videos/mcl.gif" width="70%" alt="Monte Carlo Localization simulation in Gazebo">
</center>

<hr />
<details>
<summary>üìò Read full description</summary>

<h3>Description</h3>
<p>A <strong>Monte Carlo Localization (MCL)</strong> system, also known as a <strong>Particle Filter</strong>, implemented in <strong>C++</strong> for <strong>Differential Drive Mobile Robots</strong> using <strong>ROS Noetic</strong>.</p>
<p>The algorithm estimates a robot‚Äôs pose on a known map by maintaining a set of weighted samples (‚Äúparticles‚Äù), each representing a possible state hypothesis.</p>
<hr />
<h3>Key Highlights</h3>
<ul>
<li>Reliable localization with only <strong>100 particles</strong>, compared to typical <strong>500‚Äì3000 AMCL</strong> particles.</li>
<li><strong>80% randomized resampling</strong> per iteration for fast recovery from localization loss.</li>
<li><strong>Gazebo simulation</strong> using a TurtleBot in an apartment environment.</li>
</ul>
<hr />
<h3>Frameworks &amp; Libraries</h3>
<p><img src="https://img.shields.io/badge/ROS-Noetic-blue?logo=ros" alt="ROS Noetic" /><br />
<img src="https://img.shields.io/badge/C%2B%2B-17-blue?logo=c%2B%2B" alt="C++" /><br />
<img src="https://img.shields.io/badge/Eigen-Math%20Library-lightgrey" alt="Eigen" /><br />
<img src="https://img.shields.io/badge/Gazebo-Simulation-blueviolet?logo=ros" alt="Gazebo" /><br />
<img src="https://img.shields.io/badge/RViz-Visualization-orange?logo=ros" alt="RViz" /><br />
<img src="https://img.shields.io/badge/Docker-Containerization-blue?logo=docker" alt="Docker" /></p>
<hr />
<h3>Links</h3>
<ul>
<li><a href="https://github.com/KevinEppacher/Probabilistic_Lab.git">GitHub Repository</a></li>
<li><a href="./papers/efficient_monte_carlo_localization_for_mobile_robots_implementation_and_evaluation_Eppacher.pdf">Download Research Paper (PDF)</a></li>
</ul>
<hr />
<h3>Summary</h3>
<p>Robust and efficient Monte Carlo Localization achieving high accuracy with minimal particles through adaptive resampling, enabling fast and reliable robot pose estimation in dynamic indoor environments.</p>
</details>

<hr />
<p><a id="pt-uav"></a></p>
<h2><strong>7. Design of a Cascaded Position and Velocity Controller for a Pan‚ÄìTilt Camera Tracking UAVs (Bachelor Thesis)</strong></h2>
<p>A cascaded control system enabling real-time UAV tracking with a high-speed pan‚Äìtilt camera, combining field-oriented motor control and Kalman-filtered trajectory prediction.</p>
<center>
<video width="70%" controls autoplay loop muted>
  <source src="./videos/OptoFence_Video_3.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</center>

<hr />
<details>
<summary>üìò Read full description</summary>

<h3>Description</h3>
<p>A <strong>control system for tracking UAVs</strong> using a <strong>pan‚Äìtilt camera</strong> with cascaded position and velocity control.<br />
Developed at <strong>Automation and Control Institute (TU Wien)</strong>, the system enables accurate drone tracking in real time with predictive correction via <strong>Kalman filtering</strong>.</p>
<hr />
<h3>Frameworks &amp; Libraries</h3>
<p><img src="https://img.shields.io/badge/ROS-Control-blue?logo=ros" alt="ROS" /><br />
<img src="https://img.shields.io/badge/OpenCV-Computer%20Vision-green?logo=opencv" alt="OpenCV" /><br />
<img src="https://img.shields.io/badge/C%2B%2B-17-blue?logo=c%2B%2B" alt="C++" /><br />
<img src="https://img.shields.io/badge/Python-3.10-yellow?logo=python" alt="Python" /><br />
<img src="https://img.shields.io/badge/Matlab%2FSimulink-Control-orange?logo=mathworks" alt="Matlab" /></p>
<hr />
<h3>Summary</h3>
<p>Designed a cascaded position‚Äìvelocity control system for a high-speed pan‚Äìtilt camera tracking UAVs, integrating FOC-driven PMSM motors and Kalman-filtered trajectory prediction for robust real-time tracking.</p>
</details>

<hr />
<p><a id="personal-projects"></a></p>
<h1>Personal Projects</h1>
<p>Outside of my academic research and industrial work, I enjoy building and experimenting with robotic systems in my free time, exploring mechanical design, embedded control, and intelligent motion planning.<br />
These projects allow me to prototype, test, and iterate on new ideas that blend classical robotics with modern AI-driven methods.</p>
<hr />
<h2><strong>1. 6-DOF Robotic Arm ‚Äì Design, Simulation &amp; Control</strong></h2>
<p>Designed and built a 6-DOF robotic arm using stepper-driven harmonic-drive-inspired gear reductions, integrated with ROS MoveIt for collision-aware motion planning and synchronized sim-to-real execution.</p>
<center>
<video width="70%" controls autoplay loop muted>
  <source src="./videos/6dofra.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</center>

<center>
<img src="./img/6_DOFRA_Rendered_1.png" width="45%" style="margin:10px;">
<img src="./img/6_DOFRA_Rendered_2.png" width="45%" style="margin:10px;">
</center>

<hr />
<details>
<summary>üìò Read full description</summary>

<h3>Frameworks &amp; Tools</h3>
<p><img src="https://img.shields.io/badge/ROS-Integration-blue?logo=ros" alt="ROS" /><br />
<img src="https://img.shields.io/badge/MoveIt-Motion%20Planning-purple?logo=ros" alt="MoveIt" /><br />
<img src="https://img.shields.io/badge/RViz-Visualization-orange?logo=ros" alt="RViz" /><br />
<img src="https://img.shields.io/badge/Python-3.10-yellow?logo=python" alt="Python" /><br />
<img src="https://img.shields.io/badge/C%2B%2B-17-blue?logo=c%2B%2B" alt="C++" /><br />
<img src="https://img.shields.io/badge/SolidWorks-Mechanical%20Design-red" alt="SolidWorks" /><br />
<img src="https://img.shields.io/badge/3D%20Printing-Prototyping-darkgreen" alt="3D Printing" /></p>
<hr />
<h3>Summary</h3>
<p>A 6-DOF robotic arm designed and controlled entirely through open-source tools, combining 3D-printed mechanics, ROS MoveIt motion planning, and real-to-sim synchronization for flexible robotic manipulation.</p>
</details>

<!-- END GENERATED HTML HERE -->

  </div>

<!-- END Information Cards -->
</div>

<!-- END GRID -->
</div>

<!-- END w3-content -->
</div>

<!-- Footer -->
<footer>
  <p style="margin-top:3cm;"><b>2024 Kevin Eppacher.</b>
  Powered by <u><a href="https://www.w3schools.com/w3css/default.asp" target="_blank">w3.css</a></u>.
  Icons by <u><a href="https://www.flaticon.com/icon-fonts-most-downloaded" target="_blank">flaticon.com</a></u>.
  Styling modified from <u><a href="https://github.com/KrauseFx/markdown-to-html-github-style/tree/master" target="_blank">markdown-to-html-github-style.</a></u>
  <p>This website is licensed under a <u><a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License.</a></u>
  Website source code can be borrowed, but link back to the template in your footer.</p>
</footer>

</body>
</html>
