<!DOCTYPE html>
<html>
<head>
<title>Kevin Eppacher - Github IO</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=0.9">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
<link rel="stylesheet" href="./src/styles.css">
<link rel="shortcut icon" type="image/x-icon" href="/favicon.ico"> <!--http://tools.dynamicdrive.com/favicon/-->

</head>
<body class="w3-light-grey">

<!-- w3-content defines a container for fixed size centered content, 
and is wrapped around the whole page content, except for the footer in this example -->
<div class="w3-content">

<!-- Grid -->
<div class="w3-row">

<!-- Introduction menu -->
<div class="w3-col l4">
  <!-- About Card -->
  <div class="w3-card w3-margin w3-margin-top">
    <a href="./index.html">
      <img src="./img/portrait3.jpg" style="width:100%; border-top-left-radius: 20px; border-top-right-radius: 20px;">
    </a>
    <div class="w3-container w3-white" style="border-top-left-radius: 0px; border-top-right-radius: 0px;">
      <a href="./index.html">
        <h4><b>Kevin Eppacher, B.Sc.</b></h4>
      </a>
      
      <p>Robotics Software Engineer specializing in perception-driven autonomy, semantic mapping, and intelligent control. Focused on developing systems that combine vision, optimization, and learning for real-world robotic applications.</p>

      <ul style="list-style-type:none; padding:0px; margin:0px;">
        <hr>

        <li>
          <a href="./cv/KevinEppacherCV.pdf" target="_blank">
            <button class="w3-button w3-padding-16 w3-white w3-block w3-left-align">
              <img style="height:20px; transform:translate(-50%,-2.5px);" src="./img/icons/briefcase.svg"><span><b>CV PDF Download</b></span></b>
            </button>
          </a>
        </li>

        <li>
          <a href="./portfolio.html">
            <button class="w3-button w3-padding-16 w3-white w3-block w3-left-align">
              <img style="height:20px; transform:translate(-50%,-2.5px);" src="./img/icons/square-terminal.svg"><span><b>Portfolio</b></span></b>
            </button>
          </a>
        </li>

        <hr>

        <li>
          <a href="https://github.com/KevinEppacher" target="_blank">
            <button class="w3-button w3-padding-16 w3-white w3-block w3-left-align">
              <img style="height:20px; transform:translate(-50%,-2.5px);" src="./img/icons/github.svg"><span><b>GitHub</b></span></b>
            </button>
          </a>
        </li>

        <li>
          <a href="https://www.linkedin.com/in/kevin-eppacher-4464591a0/" 
            target="_blank" 
            class="w3-button w3-padding-16 w3-white w3-block w3-left-align">
            <img style="height:20px; transform:translate(-40%,-2.5px);" src="./img/icons/linkedin.svg">
            <span><b>LinkedIn</b></span>
          </a>
        </li>

      </ul>
      <p></p>
    </div>
  </div>
  
<!-- END Introduction Menu -->
</div>

<!-- Information Cards -->
<div class="w3-col l8 s12">

  <!-- Content Class -> Holds one Publication -->
  <div class="w3-card-4 w3-margin w3-white" style="padding: 15pt;">
    
    <!-- START GENERATED HTML HERE --><h1>Professional Work</h1>
<p>Below is a selection of my research and engineering projects, spanning semantic exploration, optimization-based control, and applied industrial robotics.</p>
<h2><strong>1. Nonlinear Model Predictive Controller (nMPC) for Differential Drive Mobile Robot</strong></h2>
<center>
<video width="70%" controls autoplay loop muted>
  <source src="./videos/nMPC.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</center>

<hr />
<h3>Description</h3>
<p>A <strong>nonlinear Model Predictive Controller (nMPC)</strong> based local planner developed for a <strong>Differential Drive Mobile Robot (DDMR)</strong>.</p>
<p>Unlike conventional reactive planners, the nMPC predicts future robot states through a <strong>kinematic model</strong> and optimizes control inputs over a finite horizon.<br />
The controller minimizes a cost function while enforcing <strong>hard constraints</strong> on obstacle clearance, velocity, and input bounds.</p>
<p>The implementation leverages:</p>
<ul>
<li><strong>CasADi</strong> for nonlinear optimization formulation</li>
<li><strong>IPOPT</strong> (Interior Point Optimizer) for efficient real-time solving</li>
<li><strong>Python / ROS Noetic</strong> for seamless runtime integration</li>
</ul>
<p>The planner was benchmarked against standard local planners:</p>
<ul>
<li><strong>Dynamic Window Approach (DWA)</strong></li>
<li><strong>Timed Elastic Band (TEB)</strong></li>
</ul>
<p>Results demonstrate smoother, dynamically feasible trajectories, particularly in cluttered or narrow environments.<br />
The entire system was simulated in <strong>Gazebo</strong> using a <strong>TurtleBot</strong>, with a GPU-enabled <strong>Docker</strong> container for reproducibility.</p>
<hr />
<h3>Frameworks &amp; Libraries</h3>
<ul>
<li>ROS Noetic</li>
<li>Nav2</li>
<li>CasADi</li>
<li>IPOPT</li>
<li>Python</li>
<li>Gazebo</li>
<li>Docker</li>
</ul>
<hr />
<h3>Links</h3>
<ul>
<li><a href="https://github.com/KevinEppacher/walle_ws.git">GitHub Repository</a></li>
<li><a href="./papers/nMPC.pdf">Download unpublished Research Paper (PDF)</a></li>
</ul>
<hr />
<h2><strong>2. Automated Sensitivity Measurement System (AIRSKIN)</strong></h2>
<center>
<video width="70%" controls autoplay loop muted>
  <source src="./videos/sensibility_measurements.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</center>

<hr />
<h3>Description</h3>
<p>A <strong>collaborative project with <a href="https://www.airskin.io/">Blue Danube Robotics – AIRSKIN</a></strong> developed at <strong>UAS Technikum Vienna</strong> to automate tactile pad sensitivity measurements.</p>
<p>The system measures the <strong>force and displacement</strong> required to trigger an AIRSKIN pad at defined grid points. From this, the <strong>spring constant</strong> and <strong>local sensitivity</strong> are derived to detect mechanical weak points and support further product development.</p>
<p>Built entirely with <strong>ROS Noetic</strong> and <strong>Docker</strong>, the system integrates:</p>
<ul>
<li>A <strong>UR10</strong> industrial robot</li>
<li>A <strong>force–torque (FT) sensor</strong> connected via the UR ROS bridge (TCP/IP)</li>
<li>A <strong>custom ImGui C++ HMI</strong> for switching between <em>Freedrive Mode</em> (teaching) and <em>External Control Mode</em> (automated measurement)</li>
</ul>
<p>Once all measurement points are defined, <strong>MoveIt</strong> executes a fully automated sequence. The system visualizes force vectors in <strong>RViz</strong> and overlays a 3D point cloud from an integrated RGB-D camera, enabling intuitive analysis of pad deformation and sensitivity.</p>
<p>A full <strong>Gazebo simulation</strong> replicates the entire setup for safe testing and repeatable experiments.</p>
<hr />
<h3>Frameworks &amp; Libraries</h3>
<ul>
<li>ROS Noetic</li>
<li>MoveIt</li>
<li>ImGui (C++ GUI)</li>
<li>RViz / Gazebo</li>
<li>Docker</li>
<li>UR ROS Driver / TCP-IP Bridge</li>
</ul>
<hr />
<h3>Links</h3>
<ul>
<li><a href="https://github.com/KevinEppacher/goldilocks_sensibility_ws.git">GitHub Repository</a></li>
</ul>
<hr />
<h3>Summary</h3>
<p>Automated robotic test bench for AIRSKIN pad calibration — measuring and visualizing tactile sensitivity through force–displacement mapping.</p>
<hr />
<h2><strong>3. Monte Carlo Localization (Particle Filter) for Mobile Robots</strong></h2>
<center>
<img src="./videos/mcl.gif" width="70%" alt="Monte Carlo Localization simulation in Gazebo">
</center>

<hr />
<h3>Description</h3>
<p>A <strong>Monte Carlo Localization (MCL)</strong> system — also known as a <strong>Particle Filter</strong> — implemented in <strong>C++</strong> for <strong>Differential Drive Mobile Robots (DDMR)</strong> using <strong>ROS Noetic</strong>.</p>
<p>The algorithm estimates a robot’s pose on a known map by maintaining a set of weighted samples (“particles”), each representing a possible state hypothesis.<br />
The approach combines:</p>
<ul>
<li>A <strong>motion model</strong> for prediction (based on wheel odometry)</li>
<li>A <strong>sensor model</strong> using <strong>raycasting</strong> for probabilistic measurement updates</li>
<li>A <strong>resampling step</strong> to reinforce high-likelihood particles and discard low-likelihood ones</li>
</ul>
<hr />
<h3>Key Highlights</h3>
<ul>
<li><strong>Efficient Particle Usage:</strong> Achieved reliable localization with only <strong>100 particles</strong>, compared to typical <strong>500–3000</strong> used by <strong>AMCL</strong>, while maintaining accuracy in a small apartment map.</li>
<li><strong>Innovative Resampling Strategy:</strong> Introduced controlled randomness by regenerating <strong>80% of the particles</strong> each iteration, improving robustness against localization loss and aiding fast global convergence.</li>
<li><strong>Gazebo Simulation:</strong> Implemented and validated using a <strong>TurtleBot</strong> navigating through a custom indoor apartment environment.</li>
</ul>
<hr />
<h3>Frameworks &amp; Libraries</h3>
<ul>
<li>ROS Noetic</li>
<li>Nav2</li>
<li>C++</li>
<li>Gazebo</li>
<li>RViz</li>
<li>Eigen</li>
<li>Docker</li>
</ul>
<hr />
<h3>Links</h3>
<ul>
<li><a href="https://github.com/KevinEppacher/Probabilistic_Lab.git">GitHub Repository</a></li>
<li><a href="./papers/efficient_monte_carlo_localization_for_mobile_robots_implementation_and_evaluation_Eppacher.pdf">Download Research Paper (PDF)</a></li>
</ul>
<hr />
<h3>Summary</h3>
<p>Robust and efficient Monte Carlo Localization achieving high accuracy with minimal particles through adaptive resampling — enabling fast and reliable robot pose estimation in dynamic indoor environments.</p>
<hr />
<h2><strong>4. Design of a cascaded position and velocity controller for a pan-tilt camera tracking UAVs (Bachelor Thesis)</strong></h2>
<center>
<video width="70%" controls autoplay loop muted>
  <source src="./videos/OptoFence_Video_3.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</center>

<hr />
<h3>Description</h3>
<p>This project presents a <strong>control system for tracking UAVs at high speeds and short distances</strong> using a <strong>pan–tilt (PT) camera</strong>.<br />
Developed at the <strong>Automation and Control Institute (TU Wien)</strong>, the system addresses safety-critical scenarios by accurately tracking drone motion in real time.</p>
<p>The setup employs a <strong>cascaded position and velocity controller</strong> for the pan and tilt axes, each driven by a <strong>Permanent Magnet Synchronous Motor (PMSM)</strong> under <strong>Field-Oriented Control (FOC)</strong>.<br />
A <strong>Kalman Filter–based sensor fusion</strong> module smooths and predicts drone trajectories, ensuring continuous tracking even during temporary loss of visual contact.</p>
<p>The full system was modeled and simulated in <strong>Matlab/Simulink</strong>, <strong>OpenCV</strong> and <strong>ROS</strong>, with experiments using a <strong>laser trajectory</strong> as a reproducible target to validate controller performance.</p>
<hr />
<h3>Key Highlights</h3>
<ul>
<li>Achieved <strong>1 rad/s</strong> rotational speed, enabling tracking of drones up to <strong>30 m distance</strong> at <strong>30 m/s</strong> velocity.</li>
<li>Implemented <strong>cascaded position–velocity control</strong> for precise motor actuation.</li>
<li>Used <strong>Kalman filtering</strong> to interpolate missing detections and maintain smooth motion estimation.</li>
</ul>
<hr />
<h3>Frameworks &amp; Libraries</h3>
<ul>
<li>ROS (Robot Operating System)</li>
<li>OpenCV</li>
<li>C++</li>
<li>Python</li>
<li>Matlab/Simulink</li>
<li>OpenCV Kalman Filter (Sensor Fusion)</li>
</ul>
<hr />
<h3>Summary</h3>
<p>Developed a cascaded position–velocity control system with real-time sensor fusion for UAV tracking using a pan–tilt camera — achieving high-speed precision control and robust prediction even under visual occlusions.</p>
<h1>Personal Projects</h1>
<h2><strong>1. 6-DOF Robotic Arm – Design, Simulation, and Control</strong></h2>
<center>
<video width="70%" controls autoplay loop muted>
  <source src="./videos/6dofra.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</center>

<center>
<img src="./img/6_DOFRA_Rendered_1.png" width="45%" style="margin:10px;">
<img src="./img/6_DOFRA_Rendered_2.png" width="45%" style="margin:10px;">
</center>

<hr />
<h3>Description</h3>
<p>A fully designed and built <strong>6-DOF robotic arm</strong>, actuated by <strong>stepper motors</strong> and mechanical components such as <strong>3D-printed harmonic drives</strong> and <strong>timing belts</strong>.</p>
<p>The robot is controlled using the <strong>Robot Operating System (ROS)</strong> with the <strong>MoveIt</strong> motion planning framework.<br />
MoveIt provides both <strong>motion planning</strong> and <strong>collision avoidance</strong>, ensuring that the arm operates safely in simulation and the physical world.</p>
<p>The project demonstrates the full workflow from <strong>mechanical design</strong>, <strong>simulation</strong>, and <strong>ROS integration</strong> to <strong>hardware control</strong>.<br />
Through <strong>RViz visualization</strong>, planned trajectories are simulated before execution, and these are then transferred seamlessly to the real robot arm.</p>
<hr />
<h3>Key Highlights</h3>
<ul>
<li><strong>Custom-built 6-DOF robotic arm</strong> driven by stepper motors</li>
<li><strong>3D-printed harmonic drives</strong> and timing belt mechanisms for high precision</li>
<li><strong>MoveIt + RViz</strong> integration for planning, visualization, and collision avoidance</li>
<li><strong>ROS-based control pipeline</strong> for synchronized real-world execution</li>
<li>Modular architecture designed for future <strong>vision-based pick-and-place</strong> integration</li>
</ul>
<hr />
<h3>Frameworks &amp; Tools</h3>
<ul>
<li>ROS</li>
<li>MoveIt</li>
<li>RViz</li>
<li>Python / C++</li>
<li>3D Printing (Fusion 360 / PLA)</li>
<li>Stepper Motor Control (DRV8825 Drivers)</li>
</ul>
<hr />
<h3>Summary</h3>
<p>A 6-DOF robotic arm designed and controlled entirely through open-source tools — combining 3D-printed mechanics, ROS MoveIt motion planning, and real-to-sim synchronization for flexible robotic manipulation.</p>
<!-- END GENERATED HTML HERE -->

  </div>

<!-- END Information Cards -->
</div>

<!-- END GRID -->
</div>

<!-- END w3-content -->
</div>

<!-- Footer -->
<footer>
  <p style="margin-top:3cm;"><b>2024 Kevin Eppacher.</b>
  Powered by <u><a href="https://www.w3schools.com/w3css/default.asp" target="_blank">w3.css</a></u>.
  Icons by <u><a href="https://www.flaticon.com/icon-fonts-most-downloaded" target="_blank">flaticon.com</a></u>.
  Styling modified from <u><a href="https://github.com/KrauseFx/markdown-to-html-github-style/tree/master" target="_blank">markdown-to-html-github-style.</a></u>
  <p>This website is licensed under a <u><a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License.</a></u>
  Website source code can be borrowed, but link back to the template in your footer.</p>
</footer>

</body>
</html>
